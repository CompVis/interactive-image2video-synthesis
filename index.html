<!DOCTYPE HTML>
<!--
  Based on
	Spatial by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117339330-4"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117339330-4');
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <title>
      Understanding Object Dynamics for Interactive Image-To-Video Synthesis
    </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="landing">

		<!-- Banner -->
			<section id="banner" style="background-attachment:scroll;">
        <h2>
          Understanding Object Dynamics for Interactive Image-To-Video Synthesis
        </h2>
        <p>
        <a href="https://www.linkedin.com/in/andreas-blattmann-479038186/?originalSubdomain=de">Andreas Blattmann</a>,
        <a href="hhttps://timomilbich.github.io/">Timo Milbich</a>,
        <a href="https://mdork.github.io/">Michael Dorkenwald</a>,
        <a href="https://hci.iwr.uni-heidelberg.de/Staff/bommer">Bj&ouml;rn Ommer</a><br/>
        <a href="https://www.iwr.uni-heidelberg.de/">Interdisciplinary Center for Scientific Computing, HCI, Heidelberg University</a><br/>
        <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a><br/>
        </p>
			</section>

			<!-- One -->
				<section id="one" class="wrapper style1">
                    <div class="container 50%">
                    <div class="image fit captioned align-left"
                                style="margin-bottom:2em; box-shadow:0 0;
                                text-align:justify">
                      <img src="images/first-page.png" alt="" style="border:0px solid black"/>
                      <span style="font-weight: bold">TL;DR:</span> Our approach for interactive image-to-video synthesis learns to understand the relations between the distinct body parts of articulated objects from unlabeled video data, thus enabling synthesis of videos showing natural object dynamics as responses to local interactions.
                    </div>
                    </div>
					<div class="container 50%">

						<div class="row 200%">
							<div class="6u 12u$(medium) vert-center" style="margin:1% 0">
                  <div class="container 25%">

                    <div class="image fit captioned align-center"
                                style="margin-bottom:0em; box-shadow:0 0">
                      <a href="paper/paper.pdf">
                        <img src="paper/paper.png" alt="" style="border:1px solid black"/>
                      </a>
                        <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Blattmann_Understanding_Object_Dynamics_for_Interactive_Image-to-Video_Synthesis_CVPR_2021_paper.pdf">Paper</a>
                      <div class="headerDivider"></div>
                      <a href="paper/paper.bib">BibTeX</a>
                      <div class="headerDivider"></div>
                        <a href="https://github.com/CompVis/interactive-image2video-synthesis">GitHub</a>
                    </div>

                  </div>
							</div>
							<div class="6u$ 12u$(medium)">
                <h1>Abstract</h1>
                <p style="text-align: justify">
                What would be the effect of locally poking a static scene? We present an approach that learns naturally-looking global articulations caused by a local manipulation at a pixel level. Training requires only videos of moving objects but no information of the underlying manipulation of the physical scene. Our generative model learns to infer natural object dynamics as a response to user interaction and learns about the interrelations between different object body regions. Given a static image of an object and a local poking of a pixel, the approach then predicts how the object would deform over time. In contrast to existing work on video prediction, we do not synthesize arbitrary realistic videos but enable local interactive control of the deformation. Our model is not restricted to particular object categories and can transfer dynamics onto novel unseen object instances. Extensive experiments on diverse objects demonstrate the effectiveness of our approach compared to common video prediction frameworks.
                </p>

							</div>
						</div>
                    </div>


				</section>


			<!-- Two -->
				<section id="two" class="wrapper style2 special">
					<div class="container">
						<header class="major">
							<h2>Approach</h2>
                            <br><h3>A Hierarchical Model for Object Dynamics</h3>
						</header>

                        <div class="container 75%">
                    <div class="image fit captioned align-left"
                                style="margin-bottom:2em; box-shadow:0 0;
                                text-align:justify">
                      <img src="paper/teaser.png" alt="" style="border:0px solid black"/>
                      Left: Our framework for interactive image-to-video synthesis during training. Right: Our proposed hierarchical latent model \(\boldsymbol{\mathcal{F}}\) for synthesizing dynamics, consisting of a hierarchy of individual RNNs \(\mathcal{F}_n\), each of which operates on a different spatial feature level of the UNet defined by the pretrained encoder \(\mathcal{E}_\sigma\) and the decoder \(\mathcal{G}\). Given the initial object state \(\boldsymbol{\sigma}_0 = [\mathcal{E}_\sigma(x_0)^1,...,\mathcal{E}_\sigma(x_0)^N]\), \(\boldsymbol{\mathcal{F}}\) predicts the next state \(\boldsymbol{\hat{\sigma}}_{i+1} = [\hat{\sigma}_{i+1}^1,...,\hat{\sigma}_{i+1}^N]\) based on its current state \(\hat{\sigma}_i\) and the latent interaction \(\phi_i = \mathcal{E}_\phi(p,l)\) at the corresponding time step. The decoder \(\mathcal{G}\) finally visualizes each predicted object state \(\hat{\sigma}_i\) in an image frame \(\hat{x}_i\).
                    </div>
                        </div>
                    </div>
                </section>


        <section id="three" class="wrapper style2 special">
          <div class="container">
            <header class="major">
              <h2>Results</h2>
                <br>
              <h3>for the object categories of plants and humans.</h3>
             </header>
                <p>
                    When trained on interactions consisting of a shift \(p \in \mathbb{R}^2\) of the pixel at location \(l \in \mathbb{N}^2\), human users can apply our model to synthesize plausible object resppnses
                    to such interactions based on still images. That is, human users can define the intended target location for the poked object part while our model infers matching object dynamics
                    for the remainder of object parts.
                </p>




<header class="minor">
<h2>Plants</h2>
</header>

<p>
    As our model does not make assumptions about the objects to interact with and, thus, can be flexibly learned from unlabeled videos, it is capable to
    generate realistic looking video sequences of the distinct types of plants contained in our self-recorded PokingPlants dataset, despite their drastically varying shapes.
</p>

<div class="row 250%">
<div class="6u 12u$(xsmall)">
<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/poking_plants_1.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Results on the PP datasets for simulated pokes obtained from optical flow in the test set and also for very distinct pokes from human users.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/poking_plants_2.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Results on the PP datasets, for simulated pokes obtained from optical flow in the test set and also for very distinct pokes from human users.
</div>
</div>
</div>


<div class="row 250%">
<div class="6u 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/poking_plants_3.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Results on the PP datasets, for simulated pokes obtained from optical flow in the test set and also for very distinct pokes from human users.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/poking_plants_4.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Results on the PP datasets, for simulated pokes obtained from optical flow in the test set and also for very distinct pokes from human users.
</div>
</div>
</div>


<header class="minor">
<h2>Humans</h2>
</header>

<p>
    Also dynamics of highly-articulated objects such as humans can be learned without annotations available. Moreover, our model generalizes to unseen new instances, which have not been seen during training.
</p>

<div class="row 250%">
<div class="6u 12u$(xsmall)">
	
<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/iper_1.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Results on the iPER datasets, for simulated pokes obtained from optical flow in the test set and also for very distinct pokes from human users.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/iper_2.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Results on the iPER datasets, for simulated pokes obtained from optical flow in the test set and also for very distinct pokes from human users.
</div>
</div>
</div>

<div class="row 250%">
<div class="6u 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/iper_3.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Results on the iPER datasets, for simulated pokes obtained from optical flow in the test set and also for very distinct pokes from human users.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/iper_4.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Results on the iPER datasets, for simulated pokes obtained from optical flow in the test set and also for very distinct pokes from human users.
</div>
</div>
</div>

<header class="minor">
<h2>Understanding Object Structure</h2>
</header>


<div class="row 250%">
<div class="12u$ 12u$(xsmall)">
<div class="image fit captioned align-just">
<a href="images/object_structure.png">
<img src="images/object_structure.png" alt="" />
</a>
Understanding object structure: By performing 100 random interactions at the same location \(l\) within a given image frame \(x_0\) we obtain varying video sequences, from which we compute motion correlations for \(l\) with all remaining pixels. By mapping these correlations to the pixel space, we visualize distinct object parts.
</div>
</div>
</div>




<header class="minor">
<h2>Additional Results for Human Dynamics</h2>
</header>

<p>
    Our hierarchical model can also be applied to in-the-wild settings, as visualized by its generated sequences on the Tai-Chi dataset or to generate complex human motion such as walking sequences on the Human3.6M dataset.
</p>

<div class="row 250%">
<div class="6u 12u$(xsmall)">
	
<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/taichi_1.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Results of our model on the Tai-Chi dataset which contains many in-the-wild scenes as well as substantial amounts of background camera movements,
    thus proving our model to also be able to handle such challenging conditions.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/taichi_2.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Results of our model on the Tai-Chi dataset which contains many in-the-wild scenes as well as substantial amounts of background camera movements,
    thus proving our model to also be able to handle such challenging conditions.
</div>
</div>
</div>

<div class="row 250%">
<div class="6u 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/taichi_3.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Results of our model on the Tai-Chi dataset which contains many in-the-wild scenes as well as substantial amounts of background camera movements,
    thus proving our model to also be able to handle such challenging conditions.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/taichi_4.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Results on the Tai-Chi dataset which contains many in-the-wild scenes as well as substantial amounts of background camera movements, thus proving our model to also be able to handle such challenging conditions.
</div>
</div>
</div>

<div class="row 250%">
<div class="6u 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/h36m_1.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Results on the Human3.6M dataset, which contains complex human motion such as walking.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/h36m_2.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Results on the Human3.6M dataset, which contains complex human motion such as walking.
</div>
</div>
</div>
						</div>
				</section>


<section id="four" class="wrapper style2 special">
    <div class="container">
        <header class="major">
            <h2>Additional Applications and Experiments</h2>
        </header>
        <header class="minor">
            <h2>Interpreting the poke as initial impulse</h2>
        </header>
        <p>
            When normalizing the magnitude of pokes over the entire dataset to be in between 0 and 1, thus removing information regarding the intended target location, the poke can be interpreted as an initial force pr impulse onto
            the object part interacted with. Consequently, our model now generates sequences showing object reactions to the initial impulse defined by the interaction, where larger pokes correspond
            to similarly large amounts of object dynamics whereas interactions with a small magnitude result in subtle object motion.
        </p>

        <div class="row 250%">
            <div class="6u 12u$(xsmall)">
                <div class="image fit captioned align-just">
                    <div class="videocontainer">
                        <video controls class="videothing">
                        <source src="images/impulse_model_1.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                        </video>
                    </div>
                    Results for the interpretation of poke as an initial impulse onto the object at the respective location.
                </div>
            </div>
            <div class="6u$ 12u$(xsmall)">

                <div class="image fit captioned align-just">
                <div class="videocontainer">
                <video controls class="videothing">
                <source src="images/impulse_model_2.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
                </div>
                Results for the interpretation of poke as an initial impulse onto the object at the respective location.
                </div>
            </div>
        </div>


        <header class="minor">
            <h2>Generalization to Images obtained from web-search</h2>
        </header>
        <p>
            When combining the PP dataset and the vegetation samples from the Dynamic Textures Database, our model can be shown to generalize to images obtained from web-search.
        </p>

    <div class="row 150%">
            <div class="6u 12u$(xsmall)">
                <div class="image fit captioned align-just">
                    <div class="videocontainer">
                        <video controls class="videothing">
                        <source src="images/generalization_1.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                        </video>
                    </div>
                    Results on images obtained from web-search demonstrate the generalization capabilities of our model. In last column, we show the nearest neighbour of the respective input image in the train data.
                </div>
            </div>
            <div class="6u$ 12u$(xsmall)">

                <div class="image fit captioned align-just">
                <div class="videocontainer">
                <video controls class="videothing">
                <source src="images/generalization_3.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
                </div>
                Results on images obtained from web-search demonstrate the generalization capabilities of our model. In last column, we show the nearest neighbour of the respective input image in the train data.
                </div>
            </div>
        </div>
    <div class="row 150%">
            <div class="6u 12u$(xsmall)">
                <div class="image fit captioned align-just">
                    <div class="videocontainer">
                        <video controls class="videothing">
                        <source src="images/generalization_4.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                        </video>
                    </div>
                    Results on images obtained from web-search demonstrate the generalization capabilities of our model. In last column, we show the nearest neighbour of the respective input image in the train data.
                </div>
            </div>
            <div class="6u$ 12u$(xsmall)">

                <div class="image fit captioned align-just">
                <div class="videocontainer">
                <video controls class="videothing">
                <source src="images/generalization_2.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
                </div>
                Results on images obtained from web-search demonstrate the generalization capabilities of our model. In last column, we show the nearest neighbour of the respective input image in the train data.
                </div>
            </div>
        </div>
    </div>
</section>

			<!-- Six -->
				<section id="six" class="wrapper style3 special"
          style="background-attachment:scroll;background-position:center bottom;">
					<div class="container">
						<header class="major">
							<h2>Acknowledgement</h2>
              <p>
              	The research leading to these results is funded by the German Federal Ministry for Economic Affairs and Energy within the project “KI-Absicherung – Safe AI for automated driving” and by the German Research Foundation (DFG) within project 421703927.
              This page is based on a design by <a href="http://templated.co">TEMPLATED</a>.
              </p>
						</header>
					</div>
				</section>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
