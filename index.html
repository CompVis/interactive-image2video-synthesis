<!DOCTYPE HTML>
<!--
  Based on
	Spatial by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117339330-4"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117339330-4');
    </script>

    <title>
      Behavior-driven Synthesis of Human Dynamics
    </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="landing">

		<!-- Banner -->
			<section id="banner" style="background-attachment:scroll;">
        <h2>
          Behavior-Driven Synthesis of Human Dynamics
        </h2>
        <p>
        <a href="https://www.linkedin.com/in/andreas-blattmann-479038186/?originalSubdomain=de">Andreas Blattmann</a>&ast;, 
        <a href="hhttps://timomilbich.github.io/">Timo Milbich</a>&ast;,
        <a href="https://mdork.github.io/">Michael Dorkenwald</a>&ast;,
        <a href="https://hci.iwr.uni-heidelberg.de/Staff/bommer">Bj&ouml;rn Ommer</a><br/>
        <a href="https://www.iwr.uni-heidelberg.de/">HCI/IWR, Heidelberg University</a><br/>
        <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a><br/>
        </p>
			</section>

			<!-- One -->
				<section id="one" class="wrapper style1">
					<div class="container 75%">

						<div class="row 200%">
							<div class="6u 12u$(medium) vert-center" style="margin:1% 0">
                  <div class="container 25%">


                    <div class="image fit captioned align-center"
                                style="margin-bottom:0em; box-shadow:0 0">
                      <a href="paper/paper.pdf">
                        <img src="paper/paper.png" alt="" style="border:1px solid black"/>
                      </a>
                      <a href="https://arxiv.org/abs/2103.04677">arXiv</a>
                      <div class="headerDivider"></div>
                      <a href="paper/paper.bib">BibTeX</a>
                      <div class="headerDivider"></div>
                      <a href="https://github.com/CompVis/behavior-driven-video-synthesis">GitHub</a>
                      <br/>
                      &ast; indicates equal contribution
                    </div>

                  </div>
							</div>
							<div class="6u$ 12u$(medium)">
                <h1>Abstract</h1>
                <p style="text-align: justify">
                Generating and representing human behavior are of major importance for various computer vision applications. Commonly, human video synthesis represents behavior as sequences of postures while directly predicting their likely progressions or merely changing the appearance of the depicted persons, thus not being able to exercise control over their actual behavior during the synthesis process. In contrast, controlled behavior synthesis and transfer across individuals requires a deep understanding of body dynamics and calls for a representation of behavior that is independent of appearance and also of specific postures. In this work, we present a model for human behavior synthesis which learns a dedicated representation of human dynamics independent of postures. Using this representation, we are able to change the behavior of a person depicted in an arbitrary posture, or to even directly transfer behavior observed in a given video sequence. To this end, we propose a conditional variational framework which explicitly disentangles posture from behavior. We demonstrate the effectiveness of our approach on this novel task, evaluating capturing, transferring, and sampling fine-grained, diverse behavior, both quantitatively and qualitatively.
                </p>

							</div>
						</div>

				</section>


			<!-- Two -->
				<section id="two" class="wrapper style2 special">
					<div class="container">
						<header class="major">
							<h2>Arpproach</h2>
						</header>

            <div class="row 150%">
            	</section>

			<!-- One -->
				<section id="one" class="wrapper style1">
					<div class="container 75%">
												<div class="image fit captioned align-left"
                                style="margin-bottom:2em; box-shadow:0 0;
                                text-align:justify">
                      <img src="images/first-page.png" alt="" style="border:0px solid black"/>
                      Our Approach for Behavior Transfer. Given a source sequence of human dynamics our model infers a behavior encoding which is independent of posture. We can re-enact the behavior by combining it with an unrelated target posture and thus control the synthesis process. The resulting sequence is combined with an appearance to synthesize a video sequence.
                    </div>
                    <div class="image fit captioned align-left"
                                style="margin-bottom:2em; box-shadow:0 0;
                                text-align:justify">
                      <img src="paper/teaser.png" alt="" style="border:0px solid black"/>
                      During training we learn an behavior representation using a special conditional VAE which is independent of posture. Therefore, we use an auxiliary decoder to disentangle behavior from posture. 
                      In inference, we transfer source behavior (green) to an arbitrary target posture (yellow) or synthesize novel behavior from the prior distribution which is
						matched to q by a learned invertible transformation T (red).
                    </div>
            <div class="row 150%">
            	</section>
        <section id="two" class="wrapper style2 special">
          <div class="container">
            <header class="major">
              <h2>Results</h2>
              <p>and applications of our model.</p>
            </header>

<div class="row 150%">
<div class="6u 12u$(xsmall)">



<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/behavior_transfer1.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Behavior Transfer 1. We transfer fine-grained, characteristic body dynamics of an observed behavior x<sub>β</sub> to unrelated, significantly different target postures x<sub>t</sub> . If required, the target posture is first adjusted by a transition phase before re-enacting the inferred behavior.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/behavior_transfer1_RGB.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Behavior Transfer 1 RGB. We transfer fine-grained, characteristic body dynamics of an observed behavior x<sub>β</sub> to unrelated, significantly different target postures x<sub>t</sub> . If required, the target posture is first adjusted by a transition phase before re-enacting the inferred behavior. 
</div>
</div>
</div>


<div class="row 250%">
<div class="6u 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/behavior_transfer2.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Behavior Transfer 2. 
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/behavior_transfer2_RGB.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Behavior Transfer 2 RGB. 
</div>
</div>
</div>


<div class="row 250%">
<div class="6u 12u$(xsmall)">
	
<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/samples.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Prior Samples. We show behavior synthesis based on random sampling z<sub>β</sub> from the prior distribution
which are then transformed using T<sub>ξ</sub> . The leftmost column
depicts the target postures x<sub>t</sub> with each performing 6 ran-
domly sampled behaviors. Note, that for each target posture
x<sub>t</sub> we use different samples z<sub>β</sub>.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/nearest_neighbors.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Nearest Neighbors. To visually demonstrate that our learned representation z<sub>β</sub> actually captures
behavior dynamics while discarding posture information,
we find nearest neighbours to the ground-truth training se-
quences. Therefore, we re-enact a source behavior x<sub>β</sub> us-
ing a random target posture x<sub>t</sub>.
</div>
</div>
</div>

<div class="row 250%">
<div class="6u 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/interpolations_01.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Interpolation 1.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/interpolations_rgb_01.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Interpolation RGB 1.
</div>
</div>
</div>

<div class="row 250%">
<div class="6u 12u$(xsmall)">
	
<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/interpolations_02.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Interpolation 2.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/interpolations_rgb_02.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Interpolation RGB 2.
</div>
</div>
</div>

<div class="row 250%">
<div class="6u 12u$(xsmall)">
	
<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/interpolations_03.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Interpolation 3.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/interpolations_rgb_03.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Interpolation RGB 3.
</div>
</div>
</div>


<div class="row 150%">
<div class="6u 12u$(xsmall)">

<div class="image fit captioned align-just">
<a href="images/deepfashion_grid.jpg">
<img src="images/deepfashion_grid.jpg" alt="" />
</a>
Posture-Appearance transfer on DeepFashion. Our method can also be used to disentangle posture and appearance. A quantitative evaluation can be found Table 1 in the supplementary material.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<a href="images/market_grid.jpg">
<img src="images/market_grid.jpg" alt="" />
</a>
Posture-Appearance transfer on Market1501. Our method can also be used to disentangle posture and appearance. A quantitative evaluation can be found Table 1 in the supplementary material.

</div>
</div>
</div>

<!-- related works ! -->
<!-- 
				<section id="one" class="wrapper style1">
					<div class="container 75%">
						<div class="row 200%">
<div class="12u">
  <h4>Related Work on Modular Compositions of Deep Learning Models</h4>
</div>

<div class="12u">
  <h6>
    <a href="https://compvis.github.io/net2net/">
      Network-to-Network Translation with Conditional Invertible Neural Networks
    </a>
  </h6>
</div>
<div class="3u 12u$(medium)">
  <div class="image fit align-center">
    <a href="https://compvis.github.io/net2net/">
      <img src="https://compvis.github.io/net2net/paper/teaser.png" style="max-width:25em; margin:auto" />
    </a>
  </div>
</div>
<div class="9u 12u$(medium)">
  <p align="justify" style="line-height: 1.0em; font-size:0.8em">
  Given the ever-increasing computational costs of modern machine learning models, we need to find new ways to reuse such expert models and thus tap into the resources that have been invested in their creation. Recent work suggests that the power of these massive models is captured by the representations they learn. Therefore, we seek a model that can relate between different existing representations and propose to solve this task with a conditionally invertible network. This network demonstrates its capability by (i) providing generic transfer between diverse domains, (ii) enabling controlled content synthesis by allowing modification in other domains, and (iii) facilitating diagnosis of existing representations by translating them into interpretable domains such as images. Our domain transfer network can translate between fixed representations without having to learn or finetune them. This allows users to utilize various existing domain-specific expert models from the literature that had been trained with extensive computational resources. Experiments on diverse conditional image synthesis tasks, competitive image modification results and experiments on image-to-image and text-to-image generation demonstrate the generic applicability of our approach. For example, we translate between BERT and BigGAN, state-of-the-art text and image models to provide text-to-image generation, which neither of both experts can perform on their own.
  </p>
</div>

<div class="12u">
  <h6>
    <a href="https://compvis.github.io/invariances/">
      Making Sense of CNNs: Interpreting Deep Representations & Their Invariances with INNs
    </a>
  </h6>
</div>
<div class="3u 12u$(medium)">
  <div class="image fit align-center">
    <a href="https://compvis.github.io/invariances/">
      <img src="https://compvis.github.io/invariances/images/overview.jpg" style="max-width:25em; margin:auto" />
    </a>
  </div>
</div>
<div class="9u 12u$(medium)">
  <p align="justify" style="line-height: 1.0em; font-size:0.8em">
  To tackle increasingly complex tasks, it has become an essential ability of neural networks to learn abstract representations. These task-specific representations and, particularly, the invariances they capture turn neural networks into black box models that lack interpretability. To open such a black box, it is, therefore, crucial to uncover the different semantic concepts a model has learned as well as those that it has learned to be invariant to. We present an approach based on INNs that (i) recovers the task-specific, learned invariances by disentangling the remaining factor of variation in the data and that (ii) invertibly transforms these recovered invariances combined with the model representation into an equally expressive one with accessible semantic concepts. As a consequence, neural network representations become understandable by providing the means to (i) expose their semantic meaning, (ii) semantically modify a representation, and (iii) visualize individual learned semantic concepts and invariances. Our invertible approach significantly extends the abilities to understand black box models by enabling post-hoc interpretations of state-of-the-art networks without compromising their performance.
  </p>
</div> --> 


<!-- /related works !-->
						</div>
				</section>


			<!-- Four -->
				<section id="four" class="wrapper style3 special"
          style="background-attachment:scroll;background-position:center bottom;">
					<div class="container">
						<header class="major">
							<h2>Acknowledgement</h2>
              <p>
              	The research leading to these results is funded by the German Federal Ministry for Economic Affairs and Energy within the project “KI-Absicherung – Safe AI for automated driving” and by the German Research Foundation (DFG) within project 421703927.
              This page is based on a design by <a href="http://templated.co">TEMPLATED</a>.
              </p>
						</header>
					</div>
				</section>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
